{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColQwen Embeddings Example\n",
    "\n",
    "This notebook demonstrates how to use the ColQwen Inference service to generate embeddings for both text and images, and how to perform similarity search using these embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary dependencies and initialize our client. Make sure you have deployed the service to Modal and have the endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.client import ColpaliClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Modal endpoint URL\n",
    "MODAL_BASE_APP_URL = \"your_modal_endpoint\"\n",
    "\n",
    "colpali_client = ColpaliClient(base_url=MODAL_BASE_APP_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "Let's generate embeddings for a text query. The embeddings can be used for semantic search, similarity comparison, or other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text query\n",
    "QUERY = \"What is machine learning?\"\n",
    "\n",
    "# Generate embeddings\n",
    "result_query = await colpali_client.embed_text(QUERY)\n",
    "query_embedding = np.array(result_query)\n",
    "print(f\"Query: {QUERY}\")\n",
    "print(f\"Embedding shape: {query_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Embeddings\n",
    "\n",
    "Now let's generate embeddings for images. We'll first convert images to base64 format, then process them in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "def image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Convert a single image to base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Path to your image directory\n",
    "DOCUMENT_PATH = \"data/images\"\n",
    "\n",
    "# Get all jpg images in the directory\n",
    "image_paths = [str(p) for p in Path(DOCUMENT_PATH).glob(\"*.jpg\")]\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "\n",
    "# Convert images to base64\n",
    "base64_images = [image_to_base64(image) for image in image_paths]\n",
    "\n",
    "# Process images in batches (example with first 2 images)\n",
    "batch = base64_images[:2]\n",
    "image_embeddings = await colpali_client.embed_images(batch, batch_size=2)\n",
    "image_embedding = np.array(image_embeddings)\n",
    "print(f\"Image embeddings shape: {image_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search\n",
    "\n",
    "Now we'll implement functions to perform similarity search between text queries and images using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_multi_vector_numpy(qs: np.ndarray, ps: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute similarity scores between query and passage embeddings.\n",
    "    \n",
    "    Args:\n",
    "        qs: Query embeddings of shape (b1, n, d) or (n, d)\n",
    "        ps: Passage embeddings of shape (b2, s, d) or (s, d)\n",
    "        \n",
    "    Returns:\n",
    "        Similarity scores of shape (b1, b2)\n",
    "    \"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if qs.ndim == 2:\n",
    "        qs = qs[np.newaxis, ...]\n",
    "    if ps.ndim == 2:\n",
    "        ps = ps[np.newaxis, ...]\n",
    "\n",
    "    # Compute dot products\n",
    "    scores_4d = np.einsum(\"bnd,csd->bcns\", qs, ps)\n",
    "    \n",
    "    # Max pooling over passage dimension\n",
    "    scores_max = scores_4d.max(axis=3)\n",
    "    \n",
    "    # Sum over query dimension\n",
    "    scores_2d = scores_max.sum(axis=2)\n",
    "    \n",
    "    return scores_2d\n",
    "\n",
    "def get_top_k_images(\n",
    "    text_tokens: np.ndarray,\n",
    "    image_embeddings: np.ndarray,\n",
    "    k: int = 30,\n",
    "    threshold: float = 0.1,\n",
    ") -> list:\n",
    "    \"\"\"Get top-k most similar images for a text query.\n",
    "    \n",
    "    Args:\n",
    "        text_tokens: Query embedding of shape (m, d) or (1, m, d)\n",
    "        image_embeddings: Image embeddings of shape (N, n, d)\n",
    "        k: Number of results to return\n",
    "        threshold: Minimum similarity score threshold\n",
    "        \n",
    "    Returns:\n",
    "        List of (image_index, score) tuples sorted by score\n",
    "    \"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if text_tokens.ndim == 2:\n",
    "        text_tokens = text_tokens[np.newaxis, ...]\n",
    "    if image_embeddings.ndim == 2:\n",
    "        image_embeddings = image_embeddings[np.newaxis, ...]\n",
    "\n",
    "    # Compute similarity scores\n",
    "    scores_2d = score_multi_vector_numpy(text_tokens, image_embeddings)[0]\n",
    "\n",
    "    # Sort by score\n",
    "    sorted_by_score = sorted(enumerate(scores_2d), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter by threshold\n",
    "    filtered = [(idx, float(s)) for idx, s in sorted_by_score if s >= threshold]\n",
    "\n",
    "    return filtered[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Finally, let's visualize the top matching images for our text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k results\n",
    "top_k_results = get_top_k_images(\n",
    "    text_tokens=query_embedding,\n",
    "    image_embeddings=image_embedding,\n",
    "    k=10,\n",
    "    threshold=0.1\n",
    ")\n",
    "\n",
    "# Display results in a grid\n",
    "html = '<div style=\"display: flex; flex-wrap: wrap; gap: 20px;\">'\n",
    "for img_idx, score in top_k_results:\n",
    "    img_path = image_paths[img_idx]\n",
    "    html += f'''\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"{img_path}\" style=\"max-width: 300px; height:auto; margin: 10px;\">\n",
    "            <p>Image {img_idx + 1}<br>Score: {score:.2f}</p>\n",
    "        </div>\n",
    "    '''\n",
    "html += \"</div>\"\n",
    "\n",
    "display(HTML(html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
